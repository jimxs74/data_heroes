{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. rutin3 Load the dataset\n",
    "dataset_path = (\"surat_ctr_format_clean.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"Title\"] +\". \"+ df[\"letter_content \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df['text'].apply(preprocess)\n",
    "df[\"Title\"] = df[\"Title\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(words, n=2):\n",
    "    \"\"\"Generate ngrams from a list of words.\"\"\"\n",
    "    return [\" \".join(gram) for gram in ngrams(words, n)]\n",
    "\n",
    "def get_weighted_phrase_embedding(phrase, w2v_model, text_corpus):\n",
    "    \"\"\"\n",
    "    Generate a TF-IDF weighted averaged word embedding for a given phrase.\n",
    "    \"\"\"\n",
    "    # Generate TF-IDF dictionary\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([text_corpus])  # format text_corpus ndak bisa di masukan dalam method ini\n",
    "    tfidf_dict = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "    \n",
    "    # Get phrase embedding\n",
    "    words = phrase.split()\n",
    "    embeddings = []\n",
    "    weights = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in w2v_model.wv.key_to_index:\n",
    "            embedding = w2v_model.wv[word]\n",
    "            tfidf_weight = tfidf_dict.get(word, 1.0)  # Default weight is 1 if word not in tfidf_dict\n",
    "            embeddings.append(embedding)\n",
    "            weights.append(tfidf_weight)\n",
    "    \n",
    "    if embeddings:\n",
    "        embeddings = np.array(embeddings)\n",
    "        weights = np.array(weights)\n",
    "        weighted_average_embedding = np.average(embeddings, axis=0, weights=weights)\n",
    "        return weighted_average_embedding\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nlp_id_local.tokenizer import PhraseTokenizer \n",
    "from nlp_id_local.postag import PosTag\n",
    "\n",
    "\n",
    "model_path = \"model/train_tuned.pkl\" #add_8\n",
    "\n",
    "def detect_bigram(text, available_tokens,):\n",
    "    \n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only bigrams whose individual words are in available_tokens\n",
    "    bigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 1 and all(word in available_tokens for word in phrase.split())]\n",
    "\n",
    "    return bigrams_only\n",
    "\n",
    "def detect_trigram(text, available_tokens):\n",
    "\n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only trigrams whose individual words are in available_tokens\n",
    "    trigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 2 and all(word in available_tokens for word in phrase.split())]\n",
    "\n",
    "    return trigrams_only\n",
    "\n",
    "def get_unique_tokens_pos(all_tokens, model_path):\n",
    "    \"\"\"\n",
    "    Get unique POS tags for tokens.\n",
    "    \"\"\"\n",
    "    postagger = PosTag(model_path)\n",
    "    pos_tokens = []\n",
    "    seen_tokens = set()\n",
    "    \n",
    "    for token in all_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            tokens_pos = postagger.get_phrase_tag(token)\n",
    "            pos_tokens.append(tokens_pos)\n",
    "    return pos_tokens\n",
    "\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists into a single list.\n",
    "    \"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "\n",
    "def filter_tokens_by_pos(flat_tokens, pos_filters):\n",
    "    \"\"\"\n",
    "    Filter tokens based on their POS tags and ensure they're unique.\n",
    "    \"\"\"\n",
    "    seen_tokens = set()\n",
    "    return [token[0] for token in flat_tokens if token[1] in pos_filters and not (token[0] in seen_tokens or seen_tokens.add(token[0]))]\n",
    "\n",
    "# Function to determine if a token is a unigram, bigram, or trigram\n",
    "def get_ngram_type(token):\n",
    "    return len(token.split())\n",
    "\n",
    "def extract_keyphrases_with_ngrams_graph(text, w2v_model, judul, available_tokens, n=10):\n",
    "    # Read stopwords from the file\n",
    "    #stopwords_path = os.path.join(repo_root, \"all_stop_words.txt\")\n",
    "    stopwords_path = \"model/all_stop_words.txt\"\n",
    "    with open(stopwords_path, 'r') as file:\n",
    "        stopwords = set(file.read().strip().splitlines())\n",
    "\n",
    "    # Tokenize the text into unigrams\n",
    "    #unigrams = [word for word in text.split() if word not in stopwords]\n",
    "\n",
    "    # Tokenize the text into unigrams that are in available_tokens\n",
    "    unigrams = [word for word in text.split() if word not in stopwords and word in available_tokens]\n",
    "\n",
    "    # Generate bigrams and trigrams using nlp-id\n",
    "    bigrams = detect_bigram(text, available_tokens)\n",
    "    trigrams = detect_trigram(text, available_tokens)\n",
    "    \n",
    "    # Combine unigrams, filtered bigrams, and filtered trigrams\n",
    "    all_tokens = unigrams + bigrams + trigrams\n",
    "\n",
    "    # Filter tokens only for selected POS\n",
    "    pos_tokens = get_unique_tokens_pos(all_tokens, model_path)\n",
    "    flat_pos_tokens = flatten_list_of_lists(pos_tokens)\n",
    "    selected_pos = {'NN', 'NNP', 'VB', 'NP', 'VP'} # FW di exclude\n",
    "    filtered_tokens = filter_tokens_by_pos(flat_pos_tokens, selected_pos)\n",
    "\n",
    "    # Get embeddings for each token (averaging word embeddings for bigrams/trigrams)\n",
    "    token_embeddings = [get_weighted_phrase_embedding(token, w2v_model, text) for token in filtered_tokens]\n",
    "    \n",
    "    # Filter out tokens that don't have embeddings\n",
    "    tokens, embeddings = zip(*[(token, emb) for token, emb in zip(filtered_tokens, token_embeddings) if emb is not None])\n",
    "    # todo : masih ada token bahasa asing atau token aneh yg lolos. \n",
    "\n",
    "    # Compute the cosine similarity between token embeddings\n",
    "    cosine_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Create a graph and connect tokens with high similarity\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            if cosine_matrix[i][j] > 0.5:  # This threshold can be adjusted\n",
    "                G.add_edge(tokens[i], tokens[j], weight=cosine_matrix[i][j])\n",
    "    \n",
    "    # Create labels dictionary using the tokens\n",
    "    labels = {token: token for token in tokens}\n",
    "\n",
    "    # Compute the PageRank scores to rank the tokens\n",
    "    scores = nx.pagerank(G)\n",
    "\n",
    "    # Modify scores based on n-gram type\n",
    "    for token in scores:\n",
    "        ngram_type = get_ngram_type(token)\n",
    "        if ngram_type == 1:  # Unigram\n",
    "            pass  # No change to score\n",
    "        elif ngram_type == 2:  # Bigram\n",
    "            scores[token] *= 2  # Double the score\n",
    "        elif ngram_type == 3:  # Trigram\n",
    "            scores[token] *= 2  # Double the score\n",
    "\n",
    "    # Modify scores if token is in title letter\n",
    "    for token in scores:\n",
    "        if any(token in title for title in judul):\n",
    "            scores[token] *= 2\n",
    "\n",
    "    # Extract top N keyphrases along with their scores\n",
    "    ranked_tokens = sorted(((scores[token], token) for token in tokens if token in scores), reverse=True)\n",
    "    \n",
    "    keyphrases_with_scores = []\n",
    "    seen_tokens = set()  # Set to keep track of tokens that have already been added\n",
    "\n",
    "    for score, token in ranked_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            keyphrases_with_scores.append((token, score))\n",
    "            seen_tokens.add(token)  # Mark the token as seen\n",
    "            if len(keyphrases_with_scores) >= n:\n",
    "                break  # Stop when the desired number of keyphrases is reached\n",
    "\n",
    "    return keyphrases_with_scores, G, labels\n",
    "\n",
    "\n",
    "def visualize_graph(G, labels):\n",
    "\n",
    "    # Remove self-loops (edges that connect a node to itself)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\")\n",
    "    nx.draw_networkx_labels(G, pos, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = \"model/w2v_100/idwiki_word2vec_100_new_lower.model\"\n",
    "w2v_model = Word2Vec.load(w2v_path)\n",
    "\n",
    "# Get available tokens from the Word2Vec model\n",
    "available_tokens = set(w2v_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 0...! Done\n",
      "Processing index 1...! Done\n",
      "Processing index 2...! Done\n",
      "Processing index 3...! Done\n",
      "Processing index 4...! Done\n",
      "Processing index 5...! Done\n",
      "Processing index 6...! Done\n",
      "Processing index 7...! Done\n",
      "Processing index 8...! Done\n",
      "Processing index 9...! Done\n",
      "Processing index 10...! Done\n",
      "Processing index 11...! Done\n",
      "Processing index 12...! Done\n",
      "Processing index 13...! Done\n",
      "Processing index 14...! Done\n",
      "Processing index 15...! Done\n",
      "Processing index 16...! Done\n",
      "Processing index 17...! Done\n",
      "Processing index 18...! Done\n",
      "Processing index 19...! Done\n",
      "Processing index 20...! Done\n",
      "Processing index 21...! Done\n",
      "Processing index 22...! Done\n",
      "Processing index 23...! Done\n",
      "Processing index 24...! Done\n",
      "Processing index 25...! Done\n",
      "Processing index 26...! Done\n",
      "Processing index 27...! Done\n",
      "Processing index 28...! Done\n",
      "Processing index 29...! Done\n",
      "Processing index 30...! Done\n",
      "Processing index 31...! Done\n",
      "Processing index 32...! Done\n",
      "Processing index 33...! Done\n",
      "Processing index 34...! Done\n",
      "Processing index 35...! Done\n",
      "Processing index 36...! Done\n",
      "Processing index 37...! Done\n",
      "Processing index 38...! Done\n",
      "Processing index 39...! Done\n",
      "Processing index 40...! Done\n",
      "Processing index 41...! Done\n",
      "Processing index 42...! Done\n",
      "Processing index 43...! Done\n",
      "Processing index 44...! Done\n",
      "Processing index 45...! Done\n",
      "Processing index 46...! Done\n",
      "Processing index 47...! Done\n",
      "Processing index 48...! Done\n",
      "Processing index 49...! Done\n",
      "Processing index 50...! Done\n",
      "Processing index 51...! Done\n",
      "Processing index 52...! Done\n",
      "Processing index 53...! Done\n",
      "Processing index 54...! Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>key_4</th>\n",
       "      <th>key_5</th>\n",
       "      <th>key_6</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>score_5</th>\n",
       "      <th>score_6</th>\n",
       "      <th>score_7</th>\n",
       "      <th>score_8</th>\n",
       "      <th>score_9</th>\n",
       "      <th>score_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flare package</td>\n",
       "      <td>custody liquid</td>\n",
       "      <td>aktivitas praktik langsung</td>\n",
       "      <td>re</td>\n",
       "      <td>personil</td>\n",
       "      <td>operasi</td>\n",
       "      <td>no training</td>\n",
       "      <td>fuel</td>\n",
       "      <td>controls</td>\n",
       "      <td>compressor</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>permohonan perubahan kedua</td>\n",
       "      <td>payment milestone</td>\n",
       "      <td>target</td>\n",
       "      <td>stream sesuai</td>\n",
       "      <td>sponsor</td>\n",
       "      <td>re</td>\n",
       "      <td>april</td>\n",
       "      <td>stream</td>\n",
       "      <td>komitmen</td>\n",
       "      <td>milestone</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>penanganan limbah cair</td>\n",
       "      <td>change notice</td>\n",
       "      <td>penanganan</td>\n",
       "      <td>waste water</td>\n",
       "      <td>treatment system</td>\n",
       "      <td>limbah</td>\n",
       "      <td>tahapan</td>\n",
       "      <td>pre</td>\n",
       "      <td>commissioning commissioning</td>\n",
       "      <td>itikad baik</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        key_1              key_2                       key_3  \\\n",
       "0               flare package     custody liquid  aktivitas praktik langsung   \n",
       "1  permohonan perubahan kedua  payment milestone                      target   \n",
       "2      penanganan limbah cair      change notice                  penanganan   \n",
       "\n",
       "           key_4             key_5    key_6        key_7   key_8  \\\n",
       "0             re          personil  operasi  no training    fuel   \n",
       "1  stream sesuai           sponsor       re        april  stream   \n",
       "2    waste water  treatment system   limbah      tahapan     pre   \n",
       "\n",
       "                         key_9       key_10  score_1  score_2  score_3  \\\n",
       "0                     controls   compressor    0.051    0.041    0.033   \n",
       "1                     komitmen    milestone    0.113    0.091    0.087   \n",
       "2  commissioning commissioning  itikad baik    0.033    0.032    0.032   \n",
       "\n",
       "   score_4  score_5  score_6  score_7  score_8  score_9  score_10  \n",
       "0    0.031    0.031    0.031    0.031    0.025    0.023     0.023  \n",
       "1    0.087    0.087    0.087    0.087    0.083    0.059     0.044  \n",
       "2    0.031    0.028    0.024    0.023    0.023    0.021     0.021  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_textrank = pd.DataFrame()\n",
    "for i in df.index:\n",
    "    print('Processing index', i, end='...! ')\n",
    "    text = df[\"text\"][i] # sblm di preprocess\n",
    "    #text = df_tr[i] # setelah di preprocess\n",
    "    ls_judul = preprocess(df[\"Title\"][i]).split()\n",
    "    keyphrases,_,_ = extract_keyphrases_with_ngrams_graph(text, w2v_model, ls_judul, available_tokens, 10)\n",
    "    df_keyphrases = pd.DataFrame(keyphrases, columns=['Keyword', 'Score'])\n",
    "    a = pd.DataFrame(df_keyphrases.Keyword).T.reset_index(drop=True)\n",
    "    b = pd.DataFrame(df_keyphrases.Score).round(3).T.reset_index(drop=True)\n",
    "    df_keyphrases = pd.concat([a, b], axis=1)\n",
    "\n",
    "    # Check if there are missing columns and add them with zero values\n",
    "    missing_columns = 20 - df_keyphrases.shape[1]\n",
    "    for _ in range(missing_columns):\n",
    "        df_keyphrases[df_keyphrases.shape[1]] = 0\n",
    "\n",
    "    #df_keyphrases.columns = ['key_1', 'key_2','key_3','score_1', 'score_2','score_3']\n",
    "    df_keyphrases.columns = ['key_1', 'key_2','key_3', 'key_4', 'key_5','key_6', 'key_7', 'key_8','key_9','key_10','score_1', 'score_2','score_3','score_4', 'score_5','score_6','score_7', 'score_8','score_9','score_10'] \n",
    "    predict_textrank = pd.concat([predict_textrank, df_keyphrases], ignore_index=True)\n",
    "    print('Done')\n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_textrank.to_excel('keyword_issue.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_ina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
